import requests
from bs4 import BeautifulSoup
import csv


# Step 1: Choose a Website
url = 'https://www.scrapethissite.com/pages/'  # Replace with the URL of the website you want to scrape


# Step 2: Plan Your Scraping
# Identify the data you want to collect and the HTML structure of the website


# Step 3: Implement Web Scraping
try:
    # Make an HTTP GET request
    response = requests.get(url)
    response.raise_for_status()  # Check for any HTTP request errors

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extract and store the data
    data_to_collect = []

    # Example: Scraping article titles
    article_titles = soup.find_all('h2', class_='article-title')
    for title in article_titles:
        data_to_collect.append(title.text)

    # Example: Scraping article links
    article_links = soup.find_all('a', class_='article-link')
    links = [link['href'] for link in article_links]
    """
    [{'href': 'A.com'}, {'href': 'B.com'}] 
    """

    # Store the data in a CSV file
    with open('scraped_data.csv', 'w+', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['Title', 'Link']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader() # adds headings
        for i in range(len(data_to_collect)):
            writer.writerow({'Title': data_to_collect[i], 'Link': links[i]})

    print('Web scraping completed successfully. Data saved in "scraped_data.csv".')

except requests.exceptions.RequestException as e:
    print(f'An error occurred: {e}')



